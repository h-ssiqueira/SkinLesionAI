{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA-nf-22_XzF"
      },
      "source": [
        "# Library imports and directory mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw1Mqezw_Rtq"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "import matplotlib.pyplot as plt, numpy as np, pandas as pd, seaborn as sns, tensorflow as tf\n",
        "import cv2, gc, glob, joblib, os, random, sys, warnings, zipfile\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from skimage.transform import resize\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score, f1_score, multilabel_confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set() # for ploting\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX2RDL25D402"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/gdrive/MyDrive/PUC/TCC/Notebooks/preTreinados/kaggle.json\" \".\"\n",
        "!chmod 600 kaggle.json\n",
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mv ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8shRJTapD8Lp"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000\n",
        "with zipfile.ZipFile(\"/content/skin-cancer-mnist-ham10000.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYh8-59oECcd"
      },
      "outputs": [],
      "source": [
        "!mv /content/HAM10000_images_part_1/*.jpg /content/HAM10000_images_part_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuyoZ4-Gygv9"
      },
      "source": [
        "## Configuration variables\n",
        "\n",
        "Index | Recommended configuration\n",
        ":---: | :---:\n",
        "1 | Rotation up to 20ยบ<br>shear up to 15ยบ<br>Horizontal and vertical flip<br>Vertical and horizontal shift up to 20%<br>Zoom out up to 20%\n",
        "2 | Rotation up to 15ยบ<br>Vertical and horizontal flip<br>Image resize up to 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK0wsZE6ygCz"
      },
      "outputs": [],
      "source": [
        "#@title Configurations { display-mode: \"form\" }\n",
        "#@markdown # Environment\n",
        "csvMetadataPath = \"/content/HAM10000_metadata.csv\" #@param {type:\"string\"}\n",
        "imagesPath = \"/content/HAM10000_images_part_2/\" #@param {type:\"string\"}\n",
        "savePath = \"/content/gdrive/MyDrive/PUC/TCC/Notebooks/\" #@param {type:\"string\"}\n",
        "saveName = \"xceptionTransfBal\"        #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ## Model architecture\n",
        "architecture = \"xception\"       #@param [\"convNeXt\", \"resNetV2\", \"xception\"]\n",
        "\n",
        "#@markdown ## Height and weight of images to be crop\n",
        "#h, w = 299,299                  #@param [[224,224],[299,299]]{type:\"raw\"}\n",
        "#@markdown 299x299 if xception, else 224x224\n",
        "if architecture == \"xception\":\n",
        "    h, w = 299,299\n",
        "else:\n",
        "    h, w = 224,224\n",
        "#@markdown ## Transfer learning from imagenet\n",
        "transferLearning = True        #@param {type:\"boolean\"}\n",
        "freezeLayers = True            #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ## Remove duplicates\n",
        "#@markdown False = 10015 images, True = 7470 images\n",
        "removeDuplicates = True         #@param {type:\"boolean\"}\n",
        "balanceClasses = True           #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ## Segmentation\n",
        "segmentation = False            #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ## Data augmentation\n",
        "dataAugmentation = False        #@param {type:\"boolean\"}\n",
        "modelVersion = 1                #@param {type:\"slider\", min:1, max:6, step:1}\n",
        "\n",
        "#@markdown\n",
        "standardScalerNorm = False      #@param {type:\"boolean\"}\n",
        "minMaxScalerNorm = False        #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ## Model configuration\n",
        "epochs = 10                      #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "batchSize = 8                   #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "customOptimizer = False         #@param {type:\"boolean\"}\n",
        "useCallback = False             #@param {type:\"boolean\"}\n",
        "\n",
        "cancerType = {\n",
        "    'akiec': 0, # 0.0 - 'Doenca de Bowens'\n",
        "    'bcc': 1, # 1.0 - 'Carcinoma basocelular'\n",
        "    'bkl': 2, # 2.0 - 'Keratose benigna'\n",
        "    'df': 3, # 3.0 - 'Dermatofibroma'\n",
        "    'vasc': 4, # 4.0 - 'Lesao vascular'\n",
        "    'mel': 5, # 5.0 - 'Melanoma'\n",
        "    'nv': 6  # 6.0 - 'Nevo melanocitico'\n",
        "}\n",
        "totalClasses = len(cancerType)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_swJdui_gsF"
      },
      "source": [
        "# Dataset import (CSV Metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWRqA6em_lA8"
      },
      "outputs": [],
      "source": [
        "datasetMetadata = pd.read_csv(csvMetadataPath)\n",
        "datasetMetadata = datasetMetadata.drop(columns=['dx_type','age','sex','localization']) # Non-utilized columns\n",
        "datasetMetadata = datasetMetadata.rename(columns={\"lesion_id\": \"lesionId\",\n",
        "                                                  \"image_id\": \"imageId\",\n",
        "                                                  \"dx\": \"cancerId\"})\n",
        "datasetMetadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9THGK5kOWv-"
      },
      "outputs": [],
      "source": [
        "datasetMetadata['duplicate'] = datasetMetadata['lesionId']      # Creating a column to indicate if the lesion has more than one image\n",
        "\n",
        "uniqueLesions = datasetMetadata.groupby('lesionId').count()     # Counts the number of lesions for each id\n",
        "uniqueLesions = uniqueLesions[uniqueLesions['imageId'] == 1]    # Filter lesions which ones have unique id\n",
        "uniqueLesions.reset_index(inplace = True)                       # Add default index column\n",
        "uniqueLesions = uniqueLesions['lesionId'].values.tolist()       # Extracts the indexes into a list\n",
        "\n",
        "datasetMetadata['duplicate'] = datasetMetadata['lesionId'].apply(lambda x: False if x in uniqueLesions else True) # Fill the rows indicating if the lesion has a duplicated image\n",
        "datasetMetadata['duplicate'].value_counts()\n",
        "del uniqueLesions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE6Eucogc4Gw"
      },
      "source": [
        "Removing Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hk2vbz9x72u"
      },
      "outputs": [],
      "source": [
        "if removeDuplicates: # Adjusted to get only one of the duplicates and non duplicates\n",
        "    datasetMetadata = datasetMetadata[~((datasetMetadata['lesionId'].duplicated()) & (datasetMetadata['duplicate']))]\n",
        "datasetMetadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrYBPrkGGwIw"
      },
      "outputs": [],
      "source": [
        "labelSize = datasetMetadata.groupby('cancerId').size().sort_values() # Sort and order every cancer class\n",
        "\n",
        "ax = labelSize.plot.barh()\n",
        "\n",
        "for i, v in enumerate(labelSize):\n",
        "    ax.text(v + 3, i + .25, str(v) + ' (' + str(round(v/datasetMetadata.shape[0]*100, 2)) + '%)') # Set the percentage of each cancer class\n",
        "plt.title('Dataset Statistics')\n",
        "plt.ylabel('Cancer Type')\n",
        "plt.xlabel('Total Images')\n",
        "plt.show()\n",
        "\n",
        "print(labelSize)\n",
        "print(\"Total: \", labelSize.sum())\n",
        "del labelSize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrjawKPGCCFt"
      },
      "outputs": [],
      "source": [
        "datasetMetadata['cancerId'] = datasetMetadata['cancerId'].map(cancerType) # Map each class into an integer\n",
        "datasetMetadata.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psdovo08Q8Nu"
      },
      "source": [
        "# Dataset import (Imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNR9jkwco5Pp"
      },
      "source": [
        "Path to the dataset images\n",
        "\n",
        "\"/content/gdrive/MyDrive/PUC/TCC/Datasets/ham10000/imgs/<imgId>.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIBPw66BJMih"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "loadedImages = 0\n",
        "\n",
        "for index, row in datasetMetadata.iterrows():\n",
        "    img = cv2.imread(''.join([imagesPath, row['imageId'], '.jpg'])) # Collect the image from drive\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)                      # Invert color BGR => RGB\n",
        "    img = cv2.resize(img, (h, w))                                   # Resize the image according to params provided\n",
        "    images.append(img)\n",
        "\n",
        "    loadedImages = loadedImages + 1\n",
        "    if loadedImages % 1000 == 0:\n",
        "        print(loadedImages)\n",
        "del loadedImages, img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLbcmS9zWf0F"
      },
      "outputs": [],
      "source": [
        "plt.imshow(images[0])\n",
        "print(len(images))\n",
        "print(datasetMetadata.iloc[0])\n",
        "print(images[0].shape)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPCe8saYY5hx"
      },
      "source": [
        "# Images treatment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5fVC1hTDxmK"
      },
      "source": [
        "\n",
        "1. Split train/val/test sets\n",
        "1. Data augmentation (training set)\n",
        "1. Segmentation (all sets)\n",
        "1. Np array\n",
        "1. Normalization (all sets, fit by training)\n",
        "1. Reshape / toCategorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9q4nWYQeNU0"
      },
      "source": [
        "Spliting in training, validation and test balancing the batches by the cancer id:\n",
        "\n",
        "Train set = 70%\n",
        "\n",
        "Validation set = 10%\n",
        "\n",
        "Test set = 20%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLemfETheP3T"
      },
      "outputs": [],
      "source": [
        "XTrainVal, XTest, YTrainVal, YTest = train_test_split(images,\n",
        "                                                      datasetMetadata['cancerId'],\n",
        "                                                      test_size = 0.2,\n",
        "                                                      train_size = 0.8,\n",
        "                                                      stratify = datasetMetadata['cancerId'])\n",
        "del images, datasetMetadata\n",
        "XTrain, XVal, YTrain, YVal = train_test_split(XTrainVal,\n",
        "                                              YTrainVal,\n",
        "                                              test_size = 0.125,\n",
        "                                              train_size = 0.875,\n",
        "                                              stratify = YTrainVal)\n",
        "del XTrainVal, YTrainVal\n",
        "\n",
        "print('Train set size: ', YTrain.size)\n",
        "print('Validation set size: ', YVal.size)\n",
        "print('Test set size: ', YTest.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhbtolGj2wCm"
      },
      "source": [
        "Information of each class in every set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYZFq-dp5Zya"
      },
      "outputs": [],
      "source": [
        "labelSize = YTrain.value_counts()\n",
        "labelSize.index = labelSize.index.map(lambda x: list(cancerType.keys())[list(cancerType.values()).index(x)])\n",
        "labelSize = labelSize.sort_values()\n",
        "\n",
        "ax = labelSize.plot.barh()\n",
        "\n",
        "for i, v in enumerate(labelSize):\n",
        "    ax.text(v + 3, i + .25, str(v) + ' (' + str(round(v/YTrain.shape[0]*100, 2)) + '%)')\n",
        "plt.title('Training set')\n",
        "plt.ylabel('Cancer Type')\n",
        "plt.xlabel('Total Images')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CVQ7g2B5ZoH"
      },
      "outputs": [],
      "source": [
        "labelSize = YVal.value_counts()\n",
        "labelSize.index = labelSize.index.map(lambda x: list(cancerType.keys())[list(cancerType.values()).index(x)])\n",
        "labelSize = labelSize.sort_values()\n",
        "\n",
        "ax = labelSize.plot.barh()\n",
        "\n",
        "for i, v in enumerate(labelSize):\n",
        "    ax.text(v + 3, i + .25, str(v) + ' (' + str(round(v/YVal.shape[0]*100, 2)) + '%)')\n",
        "plt.title('Validation set')\n",
        "plt.ylabel('Cancer Type')\n",
        "plt.xlabel('Total Images')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PL4otIPEYvd"
      },
      "outputs": [],
      "source": [
        "labelSize = YTest.value_counts()\n",
        "labelSize.index = labelSize.index.map(lambda x: list(cancerType.keys())[list(cancerType.values()).index(x)])\n",
        "labelSize = labelSize.sort_values()\n",
        "\n",
        "ax = labelSize.plot.barh()\n",
        "\n",
        "for i, v in enumerate(labelSize):\n",
        "    ax.text(v + 3, i + .25, str(v) + ' (' + str(round(v/YTest.shape[0]*100, 2)) + '%)')\n",
        "plt.title('Test set')\n",
        "plt.ylabel('Cancer Type')\n",
        "plt.xlabel('Total Images')\n",
        "plt.show()\n",
        "del labelSize, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utpkQiY2dD58"
      },
      "source": [
        "Data augmentation - Only be applied in the training set\n",
        "\n",
        "Use GAN here only for class 5 (melanoma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJj6n1gjmInk"
      },
      "outputs": [],
      "source": [
        "if dataAugmentation:\n",
        "    newMelanomaImgs = 3570\n",
        "    generatorModel = tf.keras.models.load_model(''.join([savePath, '/preTreinados/generatorV', str(modelVersion), '.h5']))\n",
        "\n",
        "    generatedImages = generatorModel.predict(np.random.normal(0, 1, (newMelanomaImgs, 1, 1, 100))) # 430 + 3570 = 4000 melanoma images for training set\n",
        "    for i in range(newMelanomaImgs):\n",
        "        img = cv2.resize(generatedImages[i], (h, w)) # 256 x 256 -> model input h x w\n",
        "        XTrain.append((img * .5 + .5).astype(\"uint8\"))\n",
        "        YTrain = YTrain.append(pd.Series([5]))\n",
        "\n",
        "    del generatorModel, newMelanomaImgs, img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert len(XTrain) == len(YTrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpkTWTo6rT2D"
      },
      "outputs": [],
      "source": [
        "# if dataAugmentation:\n",
        "#     datagen = ImageDataGenerator(\n",
        "#         # Normalizations\n",
        "#         featurewise_center = featureCenter,\n",
        "#         samplewise_center = sampleCenter,\n",
        "#         featurewise_std_normalization = featureStdNorm,\n",
        "#         samplewise_std_normalization = sampleStdNorm,\n",
        "#         zca_whitening = zcaWhitening,\n",
        "#         # Image modifications\n",
        "#         rotation_range = rotation,\n",
        "#         zoom_range = zoom,\n",
        "#         channel_shift_range = channelShift,\n",
        "#         fill_mode = fill,\n",
        "#         cval = cval,\n",
        "#         width_shift_range = widthShift,\n",
        "#         height_shift_range = heightShift,\n",
        "#         horizontal_flip = hFlip,\n",
        "#         brightness_range = bright,\n",
        "#         shear_range = shear,\n",
        "#         vertical_flip = vFlip\n",
        "#     )\n",
        "#     if featureCenter or featureStdNorm or zcaWhitening:\n",
        "#         datagen.fit(XTrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEb-yUHLdwSV"
      },
      "outputs": [],
      "source": [
        "# if dataAugmentation:\n",
        "#     batch_sizes = {\n",
        "#         0: 23,\n",
        "#         1: 15,\n",
        "#         2: 6,\n",
        "#         3: 73,\n",
        "#         4: 55,\n",
        "#         5: 9,\n",
        "#         6: 0\n",
        "#     }\n",
        "#     augmented_images = []\n",
        "#     augmented_labels = []\n",
        "#     for image, label in zip(XTrain, YTrain):\n",
        "#         augmented_images.append(image)\n",
        "#         augmented_labels.append(label)\n",
        "\n",
        "#         if batch_sizes[label] > 0:\n",
        "#             image = np.expand_dims(image, axis=0)  # 3D -> 4D\n",
        "#             augmented = datagen.flow(image, batch_size=batch_sizes[label]) # Generate the image batch\n",
        "\n",
        "#             for i in range(batch_sizes[label]): # Iterate over the batch and append the images\n",
        "#                 augmented_images.append(np.squeeze(augmented.next(), axis=0).astype('uint8'))\n",
        "#                 augmented_labels.append(label)\n",
        "\n",
        "#     XTrain = augmented_images\n",
        "#     YTrain = pd.Series(augmented_labels)\n",
        "#     del augmented_labels, augmented_images, batch_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsPLNia-8AiU"
      },
      "outputs": [],
      "source": [
        "if dataAugmentation:\n",
        "    labelSize = YTrain.value_counts()\n",
        "    labelSize.index = labelSize.index.map(lambda x: list(cancerType.keys())[list(cancerType.values()).index(x)])\n",
        "    labelSize = labelSize.sort_values()\n",
        "\n",
        "    ax = labelSize.plot.barh()\n",
        "\n",
        "    for i, v in enumerate(labelSize):\n",
        "        ax.text(v + 3, i + .25, str(v) + ' (' + str(round(v/YTrain.shape[0]*100, 2)) + '%)')\n",
        "    plt.title('Training Set With Data Augmentation (GANs)')\n",
        "    plt.ylabel('Cancer Type')\n",
        "    plt.xlabel('Total')\n",
        "    plt.show()\n",
        "    del labelSize, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GdRR7cpdUtX"
      },
      "source": [
        "Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Grmnw8qdTz8"
      },
      "outputs": [],
      "source": [
        "def segmentImg(img, alpha = 1.0, kernelMask = (5, 5), kernelSoftMask = (21, 21)):\n",
        "    grayImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    softenedImg = cv2.GaussianBlur(grayImg, kernelMask, 0)                                # Apply softened filter to reduce noise\n",
        "    _, mask = cv2.threshold(softenedImg, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU) # Thresholding algorithm to create mask\n",
        "    softenedMask = cv2.GaussianBlur(mask, kernelSoftMask, 0)                              # Smoothing filter\n",
        "    segmentedImg = cv2.bitwise_and(img, img, mask = softenedMask)                         # Apply mask\n",
        "    beta = 1.0 - alpha # Beta = weight for original image, alpha = Weight for segmented image - 0 for highlight only the lesion, 1 for original image\n",
        "    return cv2.addWeighted(img, alpha, segmentedImg, beta, 0)                             # Merge segmented image and original image weighted\n",
        "\n",
        "def segmentImgList(imgs):\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = segmentImg(imgs[i])\n",
        "    return imgs\n",
        "\n",
        "if segmentation:\n",
        "    XTrain = segmentImgList(XTrain)\n",
        "    XVal = segmentImgList(XVal)\n",
        "    XTest = segmentImgList(XTest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6zYWVOXgC0J"
      },
      "source": [
        "Transform datasets into np arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C_uIIkj-Z-7"
      },
      "outputs": [],
      "source": [
        "XTrain = np.array(XTrain)\n",
        "XTrain = XTrain.reshape(XTrain.shape[0], h * w * 3) # 4d -> 2d\n",
        "\n",
        "XVal = np.array(XVal)\n",
        "XVal = XVal.reshape(XVal.shape[0], h * w * 3) # 4d -> 2d\n",
        "\n",
        "XTest = np.array(XTest)\n",
        "XTest = XTest.reshape(XTest.shape[0], h * w * 3) # 4d -> 2d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkJy_BxZdoTz"
      },
      "source": [
        "Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMHsqqWLdn8V"
      },
      "outputs": [],
      "source": [
        "if minMaxScalerNorm:\n",
        "    minMaxScaler = MinMaxScaler()\n",
        "    minMaxScaler.fit(np.asarray(XTrain))\n",
        "\n",
        "    XTrain = minMaxScaler.transform(XTrain)\n",
        "    XVal = minMaxScaler.transform(XVal)\n",
        "    XTest = minMaxScaler.transform(XTest)\n",
        "elif standardScalerNorm:\n",
        "    standardScaler = StandardScaler()\n",
        "    standardScaler.fit(np.asarray(XTrain))\n",
        "\n",
        "    XTrain = standardScaler.transform(XTrain)\n",
        "    XVal = standardScaler.transform(XVal)\n",
        "    XTest = standardScaler.transform(XTest)\n",
        "\n",
        "print(np.shape(XTest))\n",
        "print(np.shape(XTrain))\n",
        "print(np.shape(XVal))\n",
        "print(YTrain.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXbxLdHMdlvD"
      },
      "source": [
        "Resize and input adaptation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GOF05mmeT_v"
      },
      "outputs": [],
      "source": [
        "print(XTrain.shape[0])\n",
        "print(XVal.shape[0])\n",
        "print(XTest.shape[0])\n",
        "\n",
        "XTrain = XTrain.reshape(XTrain.shape[0], h, w, 3).astype('float32')\n",
        "XVal = XVal.reshape(XVal.shape[0], h, w, 3).astype('float32')\n",
        "XTest = XTest.reshape(XTest.shape[0], h, w, 3).astype('float32')\n",
        "\n",
        "YTrain = np_utils.to_categorical(YTrain)\n",
        "YVal = np_utils.to_categorical(YVal)\n",
        "YTest = np_utils.to_categorical(YTest)\n",
        "\n",
        "print(XTrain[0].shape)\n",
        "XTrain.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjwFM-cJZS3m"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABMk_62CdweF"
      },
      "source": [
        "Loading pre treined model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAN7huVP5Pf_"
      },
      "outputs": [],
      "source": [
        "with tf.device('/GPU:0'): # Using GPU processing on model\n",
        "    match architecture:\n",
        "        case \"convNeXt\":\n",
        "            model = tf.keras.applications.ConvNeXtBase(classes = totalClasses,\n",
        "                                                       include_top = False,\n",
        "                                                       input_shape = (h,w,3),\n",
        "                                                       weights = 'imagenet' if transferLearning else None)\n",
        "        case \"resNetV2\":\n",
        "            model = tf.keras.applications.ResNet50V2(classes = totalClasses,\n",
        "                                                     include_top = False,\n",
        "                                                     input_shape = (h,w,3),\n",
        "                                                     weights = 'imagenet' if transferLearning else None)\n",
        "        case \"xception\":\n",
        "            model = tf.keras.applications.Xception(classes = totalClasses,\n",
        "                                                   include_top = False,\n",
        "                                                   input_shape = (h,w,3),\n",
        "                                                   weights = 'imagenet' if transferLearning else None)\n",
        "        case _:\n",
        "            raise Exception(\"model not configured or non existent\")\n",
        "\n",
        "    if freezeLayers:\n",
        "        for layer in model.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "    model = tf.keras.Model(inputs = model.input,\n",
        "                           outputs = tf.keras.layers.Dense(totalClasses,\n",
        "                                                           activation = 'softmax')(tf.keras.layers.Flatten()(model.output)))\n",
        "\n",
        "    if customOptimizer:\n",
        "        model.compile(optimizer = AdamW(amsgrad = True,\n",
        "                          beta_1 = 0.9 if transferLearning else 0.8,\n",
        "                          beta_2 = 0.999 if transferLearning else 0.99,\n",
        "                          learning_rate = 0.00001 if transferLearning else 0.001,\n",
        "                          weight_decay = 0.04 if dataAugmentation else 0.0004),\n",
        "                      loss = \"categorical_crossentropy\",\n",
        "                      metrics = [tf.keras.metrics.Accuracy(),\n",
        "                                 tf.keras.metrics.AUC(),\n",
        "                                 tf.keras.metrics.Recall(),\n",
        "                                 tf.keras.metrics.Precision(),\n",
        "                                 tf.keras.metrics.FalsePositives(),\n",
        "                                 tf.keras.metrics.FalseNegatives(),\n",
        "                                 tf.keras.metrics.TrueNegatives(),\n",
        "                                 tf.keras.metrics.TruePositives()])\n",
        "    else:\n",
        "        model.compile(optimizer = \"adam\",\n",
        "                      loss = \"categorical_crossentropy\",\n",
        "                      metrics = [tf.keras.metrics.Accuracy(),\n",
        "                                 tf.keras.metrics.AUC(),\n",
        "                                 tf.keras.metrics.Recall(),\n",
        "                                 tf.keras.metrics.Precision(),\n",
        "                                 tf.keras.metrics.FalsePositives(),\n",
        "                                 tf.keras.metrics.FalseNegatives(),\n",
        "                                 tf.keras.metrics.TrueNegatives(),\n",
        "                                 tf.keras.metrics.TruePositives()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3kSK2-eFD26"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "plt.close('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLvmetFYj1EU"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBQlworRS5M-"
      },
      "outputs": [],
      "source": [
        "with tf.device('/GPU:0'):\n",
        "    history = model.fit(batch_size = batchSize,\n",
        "                        callbacks = [EarlyStopping(min_delta = 0.001, monitor = 'val_loss', patience=3)] if useCallback else None,\n",
        "                        class_weight = dict(enumerate(class_weight.compute_class_weight(class_weight = 'balanced',\n",
        "                                                                                        classes = np.unique(YTrain.argmax(axis=1)),\n",
        "                                                                                        y = YTrain.argmax(axis=1)))) if balanceClasses else None,\n",
        "                        epochs = epochs,\n",
        "                        validation_data = (XVal, YVal),\n",
        "                        verbose = 2,\n",
        "                        x = XTrain,\n",
        "                        y = YTrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVSWAlfrPH6F"
      },
      "outputs": [],
      "source": [
        "del XTrain, YTrain\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUCTAqwtZXq_"
      },
      "source": [
        "# Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNmDZmAyxftV"
      },
      "source": [
        "General eficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE7LL0zwxf1W"
      },
      "outputs": [],
      "source": [
        "YPredicted = model.predict(XTest)\n",
        "print('Test set evaluation: ', model.evaluate(XTest, YTest, verbose = 0))\n",
        "print('Validation set evaluation: ', model.evaluate(XVal, YVal, verbose = 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ecMbWGHewiC"
      },
      "source": [
        "Training epochs statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG7dLiHMewm8"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc = 'upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6fp7bYdzv0q"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc = 'upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YReKDHGkz3Xz"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['auc'])\n",
        "plt.plot(history.history['val_auc'])\n",
        "plt.title('Model AUC')\n",
        "plt.ylabel('AUC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc = 'upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuUAnpt0z3RI"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['recall'])\n",
        "plt.plot(history.history['val_recall'])\n",
        "plt.title('Model Recall')\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc = 'upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhBz5djBz3JR"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['precision'])\n",
        "plt.plot(history.history['val_precision'])\n",
        "plt.title('Model Precision')\n",
        "plt.ylabel('Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc = 'upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iW5m0D1ffgd"
      },
      "source": [
        "Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDiyZhCAfGEA"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(YTest.argmax(axis = 1), YPredicted.argmax(axis = 1))\n",
        "\n",
        "labels = ['Bowens disease',\n",
        "          'Basal cell carcinoma',\n",
        "          'Benign keratosis',\n",
        "          'Dermatofibroma',\n",
        "          'Vascular lesion',\n",
        "          'Melanoma',\n",
        "          'Melanocytic nevi']\n",
        "dfCm = pd.DataFrame(cm, index = labels, columns = labels)\n",
        "\n",
        "sns.set(font_scale = 1.4) # Label size\n",
        "sns.heatmap(dfCm, annot = False, cmap = \"Reds\", annot_kws = {\"size\": 14}) # Font size\n",
        "\n",
        "for i in range(totalClasses):\n",
        "    for j in range(totalClasses):\n",
        "        text = plt.text(j + 0.5, i + 0.5, f\"{cm[i, j] / cm[i].sum() * 100:.2f}%\", # {cm[i, j]:0d} ({cm[i, j] / cm[i].sum() * 100:.2f}%)\n",
        "                        va = \"center\",\n",
        "                        ha = \"center\",\n",
        "                        color = \"darkBlue\",\n",
        "                        fontsize = 10)\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation = 45, ha = 'right')\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "del ax, cm, dfCm\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXrwgJEbw539"
      },
      "source": [
        "Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVGcTl2hsM88"
      },
      "outputs": [],
      "source": [
        "metrics = {\n",
        "    'Precision': precision_score(YTest.argmax(axis=1), YPredicted.argmax(axis=1), average=None),\n",
        "    'F1-score': f1_score(YTest.argmax(axis=1), YPredicted.argmax(axis=1), average=None),\n",
        "    'ROC AUC': roc_auc_score(YTest, YPredicted, average=None),\n",
        "    'Recall': recall_score(YTest.argmax(axis=1), YPredicted.argmax(axis=1), average=None)\n",
        "}\n",
        "\n",
        "cmMulti = multilabel_confusion_matrix(YTest.argmax(axis=1), YPredicted.argmax(axis=1))\n",
        "specificity = []\n",
        "for i in range(len(cmMulti)):\n",
        "    tn = cmMulti[i][0][0]\n",
        "    specificity.append(tn / (tn + cmMulti[i][0][1]))\n",
        "metrics[f'Specificity'] = specificity\n",
        "\n",
        "metrics[f'Confusion_Matrix'] = cmMulti\n",
        "del cmMulti, specificity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOMGHwcqJpHg"
      },
      "outputs": [],
      "source": [
        "for metric, values in metrics.items():\n",
        "    print(metric + ':\\n')\n",
        "    if 'Confusion_Matrix' in metric:\n",
        "        for id, value in enumerate(values):\n",
        "          sns.set(font_scale = 1.4) # Label size\n",
        "          percentages = value / np.sum(value, axis = 1, keepdims = True)\n",
        "          sns.heatmap(value, annot = False, cmap = \"Reds\")\n",
        "          ax = plt.gca()\n",
        "          for i in range(len(value)):\n",
        "              for j in range(len(value)):\n",
        "                  ax.text(j + 0.5, i + 0.5, f'{percentages[i, j]*100:.0f}%', # {value[i, j]:0d} ({percentages[i, j]*100:.0f}%)\n",
        "                          ha = 'center',\n",
        "                          va = 'center',\n",
        "                          color = 'darkBlue',\n",
        "                          fontsize = 14)\n",
        "          ax.set_yticklabels(['False', 'True'])\n",
        "          ax.set_xticklabels(['False', 'True'])\n",
        "          plt.title(f'{labels[id]}')\n",
        "          plt.ylabel('True label')\n",
        "          plt.xlabel('Predicted label')\n",
        "          plt.show()\n",
        "    else:\n",
        "        for id, value in enumerate(values):\n",
        "            df = pd.DataFrame({'Label': labels, 'Value': values})\n",
        "        print(df.to_string(index = False, col_space = 10, justify = 'center'))\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOSa8Znhfh6u"
      },
      "source": [
        "Saving model into google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgBtttlcfIEt"
      },
      "outputs": [],
      "source": [
        "model.save(''.join([savePath, saveName, '.h5']))\n",
        "if minMaxScalerNorm:\n",
        "    joblib.dump(minMaxScaler, ''.join([savePath, saveName, 'MMscaler.pkl']))\n",
        "elif standardScalerNorm:\n",
        "    joblib.dump(standardScaler, ''.join([savePath, saveName, 'StdScaler.pkl']))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
